ğŸ”¤ Word Predictor using LSTM in PyTorch
This project demonstrates a Word Prediction Model built using an LSTM (Long Short-Term Memory) neural network in PyTorch. The model is trained on a sequence of words and learns to predict the next word based on the given context, showcasing the fundamentals of language modeling.

ğŸ§  Objective
To build a language model that takes a sequence of words as input and predicts the next likely word using LSTM, learning from text patterns in the dataset.

ğŸ“š Dataset

NLTK or spaCy (for tokenization)

ğŸ“¦ Project Structure

word-predictor-lstm/
â”‚
â”œâ”€â”€ data.txt                  # Training corpus
â”œâ”€â”€ preprocess.py             # Text preprocessing and vocabulary creation
â”œâ”€â”€ train.py                  # Model training script
â”œâ”€â”€ model.py                  # LSTM model class
â”œâ”€â”€ predict.py                # Word prediction using trained model
â”œâ”€â”€ utils.py                  # Helper functions
â””â”€â”€ README.md
âš™ï¸ How It Works
ğŸ§¹ 1. Preprocessing
Tokenize text into words

Build vocabulary (word-to-index and index-to-word mappings)

Create input-output pairs using a sliding window:

Input: ["The", "sun", "rises"] â†’ Output: "in"

ğŸ§  2. LSTM Model Architecture
class WordPredictor(nn.Module):
    def __init__(self, vocab_size, embed_dim, hidden_dim):
        super(WordPredictor, self).__init__()
        self.embedding = nn.Embedding(vocab_size, embed_dim)
        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)
        self.fc = nn.Linear(hidden_dim, vocab_size)

ğŸ” 3. Training
Loss Function: CrossEntropyLoss

Optimizer: Adam

Trained on word sequences for multiple epochs

ğŸ”® 4. Prediction
Input a sequence like: "The sun rises"

Model predicts: "in"


